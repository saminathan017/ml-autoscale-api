apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference-deployment
spec:
  replicas: 2  # Initial number of pods
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
    spec:
      containers:
      - name: ml-inference-container
        image: "192.168.49.2:5000/ml-inference-api:latest"
        imagePullPolicy: IfNotPresent   # Use local image if available
  # Image built with Docker earlier
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "250m"  # Minimum CPU to allocate per pod
            memory: "256Mi"  # Minimum memory to allocate per pod
          limits:
            cpu: "500m"  # Maximum CPU limit per pod
            memory: "512Mi"  # Maximum memory limit per pod
